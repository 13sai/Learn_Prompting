---
sidebar_position: 2
---

# ðŸŸ¢ Music Generation

Music generation models are becoming increasingly popular and are expected to have a huge impact on the music industry in the future. These models are already extremely powerful and can be used to generate any musical content from chord progressions to melodies or full songs, to structure and create music in a specific genre and to compose or improivise in the style of a particular artist.

However, despite the enormous potential of music models, it is currently difficult to find those that let us apply real prompt craft. Rather, most of the available music models are predefined in one way or another which means that the generated output is not customizable by prompts to the extent we know it from image or text generation models.
The web app Riffusion, for example, which is based on stable diffusion, can be controlled with prompts to generate instruments and pseudo styles, but uses a rigid and immutable 4/4 backbeat. Mubart, on the other hand, allows extensive prompts but seems to interpret them more in terms of a sentiment analysis that links appropriate musical stylistics to it (controlling the musical parameters in detail via prompts is not possible). 
In addition to these, there are attempts to use GPT-3 as a Text-2-Music tool with actual prompting for musical elements on the "micro-level" of notes (instead of the rather vague prompt-style-analogies mubart & riffusion produe). However, at present those attempts are limited  to single instruments.